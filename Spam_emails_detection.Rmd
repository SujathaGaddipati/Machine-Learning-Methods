---
title: "Spam V/S Nonspam emails"
author: "Sujatha"
date: "02/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__Loading required packages__
```{r loadpackages, warning=FALSE, message=FALSE}
pacman::p_load(caret, data.table, MASS, ggplot2,caTools, gains)
```

__Cleaning and loading data__
```{r dataload}
spamdata <- read.csv(url
("https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"),
header = FALSE)

columnnames <- read.csv(url
("https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"), 
comment.char="|", sep=":", header=FALSE)

colnames(spamdata)[58] = "class" 
columnnames1 <-as.matrix(columnnames[-1,-2])
colnames(spamdata)[1:57]=c(columnnames1)
spambase.dt <- as.data.table(spamdata)
```

__We will examine how each predictor differs between the spam and non-spam e-mails by comparing the spam-class average and non-spam-class average__
```{r}
##partioning dataset into spam and nonspam classes
spambase.dt_spam <- spambase.dt[(spambase.dt$class==1),]
spambase.dt_nonspam <- spambase.dt[(spambase.dt$class==0),]

## Generating mean for all variables in the two sets and comparing them
id <- seq(1,57)
spam_mean <- spambase.dt_spam[, lapply(.SD, mean),.SDcols = (1:57)]
spam_mean <- melt(spam_mean, value.name = 'spam', na.rm = FALSE)
nonspam_mean <- spambase.dt_nonspam[, lapply(.SD, mean),.SDcols = (1:57)]
nonspam_mean <- melt(nonspam_mean, value.name = 'nonspam', na.rm = FALSE)
compare_mean <- cbind(spam_mean,nonspam_mean)
compare_mean.dt <- as.data.table(compare_mean)
compare_mean.dt <- compare_mean.dt[, -3]

## Calulating the avegrage difference for all the variables between the two classes
compare_mean.dt$difference <- compare_mean.dt$spam - compare_mean.dt$nonspam

##Sorting the difference in descending order to find the top 10 difference
mean_diff_sorted <- sort.list(abs(compare_mean.dt$difference), decreasing = T)
head(mean_diff_sorted, 10)

## Creating the subset for the top 10 predictors where the mean difference is high
WorkSpambase_Data <- spamdata[,c(head(mean_diff_sorted,10),58)]
```
__Based on the above mean difference calucalation for all the variables the top 10 predictors with major difference are__ 
__1) 'capital_run_length_average',__
__2) 'capital_run_length_longest',__ 
__3) 'capital_run_length_total',__
__4) 'char_freq_!',__ 
__5) 'word_freq_hp',__
__6) 'word_freq_hpl'__
__7) 'word_freq_george'__
__8) 'word_freq_you'__
__9) 'word_freq_your'__
__10)'word_freq_free'__



__Including only 10 above identified variables, splitting & normalizing the dataset__
```{r}

WorkSpambase_Data$class <- factor(WorkSpambase_Data$class, levels = c(0,1), 
                            labels = c("Non-Spam", "Spam"))

## Partioning dataset into training and validating data set, 80% & 20% respectively
set.seed(42)
training.index <- createDataPartition(WorkSpambase_Data$class, p = 0.8, list = FALSE)
train <- WorkSpambase_Data[training.index, ]
valid <- WorkSpambase_Data[-training.index, ]

##Normalizing the dataset
norm.values  <- preProcess(train, method = c("center", "scale"))
train.norm <- predict(norm.values, train)
valid.norm <- predict(norm.values, valid)
```

__Performing a linear discriminant analysis using the training dataset.__
```{r average difference}
lda1 <- lda(class ~.,data = train.norm)
lda1
```
__Coefficients of linear discriminants are the weights for the 10 predictor variables that are calculated by the lda algorithm. Based on these weights/scores the LD1 code i.e the linear discriminants are generated for the observations.__

```{r}
##predict - using training data
pred1 <- predict(lda1, train.norm)
names(pred1)
head(pred1$class)
head(pred1$posterior)
head(pred1$x)

##predict - using validation dataset
pred2 <- predict(lda1, valid.norm)
names(pred2)
head(pred2$class)
head(pred2$posterior)
head(pred2$x)
```
__$x gives the LD1 codes i.e the linear discriminants for the indiviual observations These are calulated based on the coefficients for the 10 predictors generated by the lda function in question 2. based on the linear discriminants, posterior probablities are calculated to classify emails as spam and non spam.__


```{r}
## Generating lda plot using the training data
plot(lda1)
lda1.plot <- cbind(train.norm, predict(lda1)$x)
ggplot(lda1.plot, aes(LD1, fill = class)) + 
  geom_density(alpha = 0.2) + ggtitle('Stacked density plot for training data lda')
  

## Genrating lda plot using the valiadation dataset
lda2<-lda(class ~.,data = valid.norm)
lda2
plot(lda2)
lda2.plot <- cbind(valid.norm, predict(lda2)$x)
ggplot(lda2.plot, aes(LD1, fill = class)) + 
  geom_density(alpha = 0.2) + ggtitle('Stacked density plot for validation data lda')
```
__In the above code we generated histogram and stacked density plots for the lda generated using training and validation data set.From the LDA plots of training and validation dataset we can see that there is a lot of overlap between the spam and non-spam emails and hence the separation achieved by lda is not accurate enough. For both the plots there is no pure classifcation.The spread and the distribution between the training and validation dataset differs__

```{r}
# Confusion matrix
predict_acc <- table(pred2$class, valid.norm$class)
confusionMatrix(predict_acc)
```
__Sensitivity = 0.901. It measures how accurately our model classifies nonspam emails as nonspam correctly.__
__Specificity 0.674. It measures how accurately our model classifies spam emails as spam correctly.__


```{r}
### lift chart
valid.norm$spam <- ifelse(valid.norm$class == 'Spam', 1, 0 )
gain <- gains(valid.norm$spam, pred2$posterior[,2], groups = 10)

plot(c(0,gain$cume.pct.of.total*sum(valid.norm$spam))~c(0,gain$cume.obs), 
     xlab = "# cases", ylab = "Cumulative", type = "l")
lines(c(0,sum(valid.norm$spam))~c(0, dim(valid.norm)[1]),col = "gray", lty = 2)

## Declie chart
gain <- gains(valid.norm$spam, pred2$posterior[,2])
barplot(gain$mean.resp/mean(valid.norm$spam), names.arg = gain$depth, space = 0.5,
        xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart",
        col = "coral1", border = NA)
```
__At any given point in lift curve, it tells us how much better our model is doing with respect to the random assignment. If we use our model to choose the top 400 spam email cases, lift curve tells us that we will be right for about close to 290 - 300 of them. From the decile chart it indicates that we can use the model to select the top 30% records with the highest propensities and still perform twice as well as random.For the 20% decile our model performs much better than for the first 10% decile__
